{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d5fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca208cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.index  import DirectoryReader\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.search.similarities import BM25Similarity\n",
    "from org.apache.lucene.search.similarities import LMDirichletSimilarity\n",
    "from org.apache.lucene.search.similarities import LMJelinekMercerSimilarity\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "import os,sys\n",
    "from java.nio.file import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb162ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.index import IndexWriter , IndexWriterConfig\n",
    "from org.apache.lucene.store import SimpleFSDirectory , FSDirectory\n",
    "import org.apache.lucene.document as document \n",
    "from org.apache.lucene.document import Document, Field\n",
    "from java.io import File\n",
    "import json\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a789fe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7faa4c0d9bb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c4f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preproc(query):\n",
    "    tokens=word_tokenize(query)\n",
    "    #print(tokens)\n",
    "    filtered_text=\"\"\n",
    "    for token in tokens:\n",
    "        #print(\"Token\",token)\n",
    "        if re.search('[a-zA-Z]',token):\n",
    "            if (('http' not in token) and ('@' not in token) and ('<.*?>' not in token) and ('*'not in token)):\n",
    "                token=ps.stem(token)\n",
    "                token=token.strip(\"/\")\n",
    "                token=token.replace(\"'\",\" \")\n",
    "                filtered_text=filtered_text+\" \"+token\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18c2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "Docid=[]\n",
    "def search_BM_25(index_path,q,r,k,b):\n",
    "    analyzer=EnglishAnalyzer()\n",
    "    directory=FSDirectory.open(File(index_path).toPath())\n",
    "    searcher=IndexSearcher(DirectoryReader.open(directory))\n",
    "    #k=float(input(\"enter the value of k\"))\n",
    "    #b=float(input(\"enter the value of b\"))\n",
    "    searcher.setSimilarity(BM25Similarity(k,b))\n",
    "    query=QueryParser(\"Content\",analyzer).parse(q)\n",
    "    scoreDocs=searcher.search(query,5).scoreDocs\n",
    "    t=0\n",
    "    for scoreDoc in scoreDocs:\n",
    "        if(t<scoreDoc.score):\n",
    "            t=scoreDoc.score\n",
    "    #output=[]\n",
    "    for scoreDoc in scoreDocs:\n",
    "        doc=searcher.doc(scoreDoc.doc)\n",
    "        \n",
    "        if doc.get(\"Docid\") not in Docid:\n",
    "            importance=dict()\n",
    "            Docid.append(doc.get(\"Docid\"))\n",
    "            importance[\"requestID\"]=doc.get(\"RequestId\")\n",
    "            string=doc.get(\"Facttext\")\n",
    "            string=string.encode(\"ascii\", \"ignore\")\n",
    "            string=string.decode()\n",
    "            importance[\"factText\"]=string[0:200]\n",
    "            importance[\"unixTimestamp\"]=int(doc.get(\"Timestamp\"))\n",
    "            importance[\"importance\"]=scoreDoc.score\n",
    "            importance[\"sources\"]=[doc.get(\"Docid\")]\n",
    "            importance[\"streamID\"]=doc.get(\"Docid\")\n",
    "            importance[\"informationNeeds\"]=[r]\n",
    "            output.append(importance)\n",
    "    \n",
    "   \n",
    "        \n",
    "#         print(scoreDoc.score,t,scoreDoc.score/t)\n",
    "#     print(\"Number of documnets matched\",len(scoreDocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f084886",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "def search_LM_DS(index_path,q,r,MU):\n",
    "    analyzer=EnglishAnalyzer()\n",
    "    directory=FSDirectory.open(File(index_path).toPath())\n",
    "    searcher=IndexSearcher(DirectoryReader.open(directory))\n",
    "    #MU=float(input(\"enter the value of MU\"))\n",
    "    searcher.setSimilarity(LMDirichletSimilarity(MU))\n",
    "    query=QueryParser(\"Content\",analyzer).parse(q)\n",
    "    scoreDocs=searcher.search(query,5).scoreDocs\n",
    "    for scoreDoc in scoreDocs:\n",
    "        doc=searcher.doc(scoreDoc.doc)\n",
    "        \n",
    "        if doc.get(\"Docid\") not in Docid:\n",
    "            importance=dict()\n",
    "            Docid.append(doc.get(\"Docid\"))\n",
    "            importance[\"requestID\"]=doc.get(\"RequestId\")\n",
    "            string=doc.get(\"Facttext\")\n",
    "            string=string.encode(\"ascii\", \"ignore\")\n",
    "            string=string.decode()\n",
    "            importance[\"factText\"]=string[0:200]\n",
    "            importance[\"unixTimestamp\"]=int(doc.get(\"Timestamp\"))\n",
    "            importance[\"importance\"]=scoreDoc.score\n",
    "            importance[\"sources\"]=[doc.get(\"Docid\")]\n",
    "            importance[\"streamID\"]=doc.get(\"Docid\")\n",
    "            importance[\"informationNeeds\"]=[r]\n",
    "            output.append(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26eef6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "Docid=[]\n",
    "def search_LM_JM(index_path,q,r,LAMBDA):\n",
    "    analyzer=EnglishAnalyzer()\n",
    "    directory=FSDirectory.open(File(index_path).toPath())\n",
    "    searcher=IndexSearcher(DirectoryReader.open(directory))\n",
    "    #k=float(input(\"enter the value of k\"))\n",
    "    #b=float(input(\"enter the value of b\"))\n",
    "    searcher.setSimilarity(LMJelinekMercerSimilarity(LAMBDA))\n",
    "    query=QueryParser(\"Content\",analyzer).parse(q)\n",
    "    \n",
    "    scoreDocs=searcher.search(query,5).scoreDocs\n",
    "    t=0\n",
    "    for scoreDoc in scoreDocs:\n",
    "        if(t<scoreDoc.score):\n",
    "            t=scoreDoc.score\n",
    "    #output=[]\n",
    "    for scoreDoc in scoreDocs:\n",
    "        doc=searcher.doc(scoreDoc.doc)\n",
    "        \n",
    "        if doc.get(\"Docid\") not in Docid:\n",
    "            importance=dict()\n",
    "            Docid.append(doc.get(\"Docid\"))\n",
    "            importance[\"requestID\"]=doc.get(\"RequestId\")\n",
    "            string=doc.get(\"Content\")\n",
    "            string=string.encode(\"ascii\", \"ignore\")\n",
    "            string=string.decode()\n",
    "            importance[\"factText\"]=string[0:200]\n",
    "            importance[\"unixTimestamp\"]=int(doc.get(\"Timestamp\"))\n",
    "            importance[\"importance\"]=scoreDoc.score\n",
    "            importance[\"sources\"]=[doc.get(\"Docid\")]\n",
    "            importance[\"streamID\"]=doc.get(\"Docid\")\n",
    "            importance[\"informationNeeds\"]=[r]\n",
    "            output.append(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf3f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output1= pd.DataFrame(output)\n",
    "# output1[\"importance\"] = output1[\"importance\"] / output1[\"importance\"].max()\n",
    "# with open(\"./test3.json\", \"a\") as out_file:\n",
    "#     for idx,row in output1\\\n",
    "#         .iterrows():\n",
    "        \n",
    "#         out_file.write(\"%s\\n\" % json.dumps(dict(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ba71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def retrieve(queryAsDataFrame,indexPath):\n",
    "    Docid.clear()\n",
    "    k=1.2\n",
    "    b=0.75\n",
    "    #LAMBDA=0.6\n",
    "    j=0\n",
    "    #MU=0.6\n",
    "    while(j<len(queryAsDataFrame)):\n",
    "        #print(queryAsDataFrame['text'][j])\n",
    "        sent=text_preproc(queryAsDataFrame['text'][j])\n",
    "        #print(sent)\n",
    "        r=queryAsDataFrame['query_id'][j]\n",
    "        #search_LM_DS(indexPath,sent,r,MU)\n",
    "        search_BM_25(indexPath,sent,r,k,b)\n",
    "        j+=1\n",
    "    output1= pd.DataFrame(output)\n",
    "    output.clear()\n",
    "    output1[\"importance\"] = output1[\"importance\"] / output1[\"importance\"].max()\n",
    "    with open(\"./test11.json\", \"a\") as out_file:\n",
    "        for idx,row in output1\\\n",
    "             .iterrows():\n",
    "        \n",
    "            out_file.write(\"%s\\n\" % json.dumps(dict(row)))\n",
    "\n",
    "    \n",
    "#     json_object = json.dumps(output, indent=4)\n",
    "#     with open(\"test.json\", \"a\") as outfile:\n",
    "#         outfile.write(json_object)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf2e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(event,indexPath):\n",
    "    dataset = ir_datasets.load(event)\n",
    "    queryAsDataFrame=pd.DataFrame(dataset.queries_iter())\n",
    "    retrieve(queryAsDataFrame,indexPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4bb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventNoList = [\n",
    "          \"001\", # Lilac Wildfire 2017\n",
    "          \"002\", # Cranston Wildfire 2018\n",
    "          \"003\", # Holy Wildfire 2018\n",
    "          \"004\", # Hurricane Florence 2018\n",
    "          \"005\", # 2018 Maryland Flood\n",
    "          \"006\", # Saddleridge Wildfire 2019\n",
    "          \"007\", # Hurricane Laura 2020\n",
    "          \"008\" # Hurricane Sally 2020\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ff983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Gets the list of days for a specified event number, e.g. '001'\n",
    "def getDaysForEventNo(eventNo):\n",
    "\n",
    "  # We will download a file containing the day list for an event\n",
    "  url = \"http://trecis.org/CrisisFACTs/CrisisFACTS-\"+eventNo+\".requests.json\"\n",
    "\n",
    "  # Download the list and parse as JSON\n",
    "  dayList = requests.get(url).json()\n",
    "\n",
    "  # Print each day\n",
    "  # Note each day object contains the following fields\n",
    "  #   {\n",
    "  #      \"eventID\" : \"CrisisFACTS-001\",\n",
    "  #      \"requestID\" : \"CrisisFACTS-001-r3\",\n",
    "  #      \"dateString\" : \"2017-12-07\",\n",
    "  #      \"startUnixTimestamp\" : 1512604800,\n",
    "  #      \"endUnixTimestamp\" : 1512691199\n",
    "  #   }\n",
    "\n",
    "  return dayList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0941fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 001\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "Event 002\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "Event 003\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "Event 004\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "Event 005\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "Event 006\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "Event 007\n",
      "##\n",
      "##\n",
      "Event 008\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "for eventNo in eventNoList: # for each event\n",
    "    dayList = getDaysForEventNo(eventNo) # get the list of days\n",
    "    print(\"Event \"+eventNo)\n",
    "    #print(dayList[0]['requestID'])\n",
    "    for day in dayList: # for each day\n",
    "        event=\"crisisfacts/\"+eventNo+\"/\"+day[\"dateString\"]\n",
    "        path=\"crisisfacts_\"+str(eventNo)+\"_\"+str(day[\"dateString\"])\n",
    "        parent_dir=\"/home/subinay/Trec 2022/CrisisFacts/index/\"\n",
    "        indexPath=os.path.join(parent_dir,path)\n",
    "        query(event,indexPath)\n",
    "        print(\"##\")\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875d3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfc0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
